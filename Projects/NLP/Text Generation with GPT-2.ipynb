{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPGp8ukJRJWZzSWqSJYI846"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eIpFxtG06iVq"},"outputs":[],"source":["pip install transformers torch"]},{"cell_type":"code","source":[" Import the Necessary Libraries"],"metadata":{"id":"ag5lOzT16u9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the Pre-trained GPT-2 Model and Tokenizer"],"metadata":{"id":"1XjvHq556x-0"}},{"cell_type":"code","source":["# Load the pre-trained GPT-2 model and tokenizer\n","model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"XLUkiMtC6xiW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Text Generation Function"],"metadata":{"id":"jcHGvtIp62j0"}},{"cell_type":"code","source":["def generate_text(prompt, max_length=50, temperature=1.0, top_k=50, top_p=0.95):\n","    # Encode the prompt text\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","\n","    # Generate text using the model\n","    with torch.no_grad():\n","        output = model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            temperature=temperature,  # Controls randomness\n","            top_k=top_k,              # Limits sampling to top-k tokens\n","            top_p=top_p,              # Nucleus sampling to top-p tokens\n","            do_sample=True,           # Use sampling instead of greedy decoding\n","            pad_token_id=tokenizer.eos_token_id  # To avoid warnings for GPT-2\n","        )\n","\n","    # Decode and return the generated text\n","    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return generated_text"],"metadata":{"id":"5a-DIzld671a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"kq474qzo6-Pp"}},{"cell_type":"code","source":["prompt = \"Once upon a time\"\n","generated_text = generate_text(prompt, max_length=100)\n","\n","print(\"Generated Text:\")\n","print(generated_text)"],"metadata":{"id":"Zs9zcZNs7Aub"},"execution_count":null,"outputs":[]}]}